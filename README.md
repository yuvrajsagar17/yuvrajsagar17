<!--
**yuvrajsagar17/yuvrajsagar17** is a âœ¨ _special_ âœ¨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- ðŸ”­ Iâ€™m currently working on ...
- ðŸŒ± Iâ€™m currently learning ...
- ðŸ‘¯ Iâ€™m looking to collaborate on ...
- ðŸ¤” Iâ€™m looking for help with ...
- ðŸ’¬ Ask me about ...
- ðŸ“« How to reach me: ...
- ðŸ˜„ Pronouns: ...
- âš¡ Fun fact: ...
-->

## ðŸ‘‹ Hi there,

I'm a Machine Learning Engineer with a strong focus on optimizing Large Language Models (LLMs) and Deep Learning Frameworks. I like fine-tuning, merging and evaluating LLM models. Inspired from [llm.c](https://github.com/karpathy/llm.c) by **Karpathy**, I also love to explore and maximizing the use of Nvidia GPUs through custom CUDA kernel optimization.

# ðŸ”¬ My current work includes:

- **axolotl-finetune**: In this project, I've implemented simple and multi-GPU finetuning for LLaMA models, conducted Nous evaluation benchmarks, and will soon integrate model quantization techniques.
- **llama.c**: Implemented LLama3 architecture using custom CUDA C/C++ kernels to attain high-performance for model pretraining on Nvidia GPUs.

ðŸš€ I'm also deeply interested in cutting-edge ML research, particularly in the evolution of LLMs and improving their pre-training efficiency.

Feel free to explore my work and repositories!

---

@ [GitHub](https://github.com/yourusername) @ [LinkedIn](https://www.linkedin.com/in/yuvraj-sagar-514806227/)
